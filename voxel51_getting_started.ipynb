{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7a1782a1",
      "metadata": {},
      "source": [
        "# DINOv3 visual search with FiftyOne\n",
        "\n",
        "End-to-end notebook following https://docs.voxel51.com/tutorials/dinov3.html: install, authenticate to Hugging Face, load data, extract DINOv3 embeddings, visualize and search, train a small classifier, and generate PCA/CLS foreground masks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f823cf3",
      "metadata": {},
      "source": [
        "## 1. Install required libraries\n",
        "\n",
        "Uncomment if needed. Requires network access for pip and dataset downloads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "029c465f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install git+https://github.com/huggingface/transformers\n",
        "# !pip install -q huggingface_hub\n",
        "# !pip install fiftyone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1fbc15",
      "metadata": {},
      "source": [
        "## 2. Log in to Hugging Face\n",
        "Needed to access gated models like DINOv3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6a0f949c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecff48c7",
      "metadata": {},
      "source": [
        "## 3. Load a dataset\n",
        "Uses COCO validation via FiftyOne Zoo; swap with your dataset path if desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0551d924",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 images under /Data\n",
            " 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 0/0 [1.7ms elapsed, ? remaining, ? samples/s]  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Name:        whale-local-dinov3\n",
              "Media type:  None\n",
              "Num samples: 0\n",
              "Persistent:  True\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:               fiftyone.core.fields.ObjectIdField\n",
              "    filepath:         fiftyone.core.fields.StringField\n",
              "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
              "    created_at:       fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at: fiftyone.core.fields.DateTimeField"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from pathlib import Path\n",
        "\n",
        "# Point to your local image root. All JPGs in subfolders will be ingested.\n",
        "data_root = Path(\"/home/trudes/Projects/Dinov3-WhaleID/Data\")\n",
        "image_paths = sorted([str(p) for p in data_root.rglob(\"*.jpg\")])\n",
        "print(f\"Found {len(image_paths)} images under {data_root}\")\n",
        "\n",
        "# Clean up any empty prior dataset to avoid MediaTypeError\n",
        "if fo.dataset_exists(\"whale-local-dinov3\"):\n",
        "    fo.delete_dataset(\"whale-local-dinov3\")\n",
        "\n",
        "# Create an image-only dataset from local files\n",
        "dataset = fo.Dataset.from_images(image_paths, name=\"whale-local-dinov3\")\n",
        "dataset.persistent = True\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a40ba3",
      "metadata": {},
      "source": [
        "## 4. Build the DINOv3 model wrapper\n",
        "Creates a transformers model and wraps it for FiftyOne embedding extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5751d7ee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2832e1c30d84b0989a6860692528c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/415 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import transformers\n",
        "import fiftyone.utils.transformers as fouhft\n",
        "\n",
        "transformers_model = transformers.AutoModel.from_pretrained(\"facebook/dinov3-vitl16-pretrain-lvd1689m\")\n",
        "model_config = fouhft.FiftyOneTransformerConfig(\n",
        "    {\"model\": transformers_model, \"name_or_path\": \"facebook/dinov3-vitl16-pretrain-lvd1689m\"}\n",
        ")\n",
        "model = fouhft.FiftyOneTransformer(model_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c43d953",
      "metadata": {},
      "source": [
        "## 5. Compute embeddings\n",
        "Writes embeddings to `embeddings_dinov3`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "802571d8",
      "metadata": {},
      "outputs": [
        {
          "ename": "MediaTypeError",
          "evalue": "Unsupported media type 'None'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMediaTypeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_field\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings_dinov3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Dinov3-WhaleID/.venv/lib/python3.12/site-packages/fiftyone/core/collections.py:3701\u001b[39m, in \u001b[36mSampleCollection.compute_embeddings\u001b[39m\u001b[34m(self, model, embeddings_field, batch_size, num_workers, skip_failures, progress, **kwargs)\u001b[39m\n\u001b[32m   3634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_embeddings\u001b[39m(\n\u001b[32m   3635\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3636\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3642\u001b[39m     **kwargs,\n\u001b[32m   3643\u001b[39m ):\n\u001b[32m   3644\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Computes embeddings for the samples in the collection using the\u001b[39;00m\n\u001b[32m   3645\u001b[39m \u001b[33;03m    given model.\u001b[39;00m\n\u001b[32m   3646\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3699\u001b[39m \u001b[33;03m            embeddings were computed at all\u001b[39;00m\n\u001b[32m   3700\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfomo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3702\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3704\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings_field\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3705\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3707\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_failures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_failures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3709\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3710\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Dinov3-WhaleID/.venv/lib/python3.12/site-packages/fiftyone/core/models.py:986\u001b[39m, in \u001b[36mcompute_embeddings\u001b[39m\u001b[34m(samples, model, embeddings_field, batch_size, num_workers, skip_failures, progress, **kwargs)\u001b[39m\n\u001b[32m    984\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m fom.SelectGroupSlicesError((fom.IMAGE, fom.VIDEO))\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m samples.media_type != fom.VIDEO:\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m fom.MediaTypeError(\n\u001b[32m    987\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsupported media type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % samples.media_type\n\u001b[32m    988\u001b[39m     )\n\u001b[32m    990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model.media_type == \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m samples.media_type != fom.VIDEO:\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mVideo models can only be applied to video collections\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
            "\u001b[31mMediaTypeError\u001b[39m: Unsupported media type 'None'"
          ]
        }
      ],
      "source": [
        "dataset.compute_embeddings(model, embeddings_field=\"embeddings_dinov3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7949909",
      "metadata": {},
      "source": [
        "## 6. Visualize embeddings (UMAP)\n",
        "Compute a dense UMAP and store in the brain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9354eb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "viz = fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"embeddings_dinov3\",\n",
        "    brain_key=\"dino_dense_umap\",\n",
        ")\n",
        "viz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9843438",
      "metadata": {},
      "source": [
        "## 7. Launch App and explore\n",
        "Start the FiftyOne App, then compute similarity and view nearest neighbors to a query sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9686396e",
      "metadata": {},
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, port=5151)\n",
        "session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0a7a0e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(session.url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd75d63f",
      "metadata": {},
      "outputs": [],
      "source": [
        "idx = fob.compute_similarity(\n",
        "    dataset,\n",
        "    embeddings=\"embeddings_dinov3\",\n",
        "    metric=\"cosine\",\n",
        "    brain_key=\"dino_sim\",\n",
        ")\n",
        "idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59753a82",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_id = dataset.first().id\n",
        "view = dataset.sort_by_similarity(query_id, k=20)\n",
        "session.view = view\n",
        "view\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa09b84",
      "metadata": {},
      "source": [
        "## 8. Classification from embeddings\n",
        "Derive image-level labels from detections, train a linear head, run inference, and evaluate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40417e36",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Auto-generate image-level labels from detections if present; otherwise skip\n",
        "has_detections = dataset.has_field(\"detections\")\n",
        "ids        = dataset.values(\"id\")\n",
        "paths      = dataset.values(\"filepath\")\n",
        "embs       = dataset.values(\"embeddings_dinov3\")\n",
        "\n",
        "if has_detections:\n",
        "    det_lists  = dataset.values(\"detections.detections.label\")\n",
        "    img_labels = [Counter(L).most_common(1)[0][0] if L else None for L in det_lists]\n",
        "    dataset.set_values(\n",
        "        \"image_label\",\n",
        "        [fo.Classification(label=l) if l is not None else None for l in img_labels],\n",
        "    )\n",
        "    print(\"Image labels populated from detections.\")\n",
        "else:\n",
        "    img_labels = [None for _ in ids]\n",
        "    print(\"No detections field; skipping auto image labels. Add detections to use this section.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0ad357",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare train data if labels exist\n",
        "if any(img_labels):\n",
        "    mask = [(x is not None) and (y is not None) for x, y in zip(embs, img_labels)]\n",
        "    X = normalize(np.stack([x for x,m in zip(embs,mask) if m], axis=0))\n",
        "    y = [lab for lab,m in zip(img_labels,mask) if m]\n",
        "    print(f\"Training samples: {len(y)}\")\n",
        "else:\n",
        "    X, y = None, None\n",
        "    print(\"No labels available; skip classifier training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce77b16",
      "metadata": {},
      "outputs": [],
      "source": [
        "if X is not None and len(y) > 1:\n",
        "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1).fit(X, y)\n",
        "else:\n",
        "    clf = None\n",
        "    print(\"Classifier not trained (need at least 2 labeled samples).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93e8de7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if clf is None:\n",
        "    print(\"Skipping inference; classifier not trained.\")\n",
        "else:\n",
        "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
        "        v = sample[\"embeddings_dinov3\"]\n",
        "        if v is None:\n",
        "            continue\n",
        "\n",
        "        Xpred = normalize(np.asarray(v, dtype=np.float32).reshape(1, -1))\n",
        "        p = clf.predict_proba(Xpred)[0]\n",
        "        k = int(np.argmax(p))\n",
        "\n",
        "        sample[\"predict_dinov3\"] = fo.Classification(\n",
        "            label=str(clf.classes_[k]),\n",
        "            confidence=float(p[k]),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412dcdef",
      "metadata": {},
      "outputs": [],
      "source": [
        "if clf is None:\n",
        "    print(\"Skipping evaluation; classifier not trained.\")\n",
        "else:\n",
        "    results = dataset.evaluate_classifications(\n",
        "        \"predict_dinov3\", gt_field=\"image_label\", method=\"simple\", eval_key=\"dino_simple\"\n",
        "    )\n",
        "    results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb7c54e",
      "metadata": {},
      "source": [
        "## 9. PCA/CLS foreground segmentation\n",
        "Helper to compute CLS/foreground masks and optional heatmaps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110ad8f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from fiftyone import Segmentation, Heatmap\n",
        "\n",
        "def build_pca_fg_masks(\n",
        "    dataset: fo.Dataset,\n",
        "    model_id: str = \"facebook/dinov3-vits16-pretrain-lvd1689m\",\n",
        "    field: str = \"pca_fg\",\n",
        "    heatmap_field: str | None = None,\n",
        "    thresh: float = 0.5,\n",
        "    smooth_k: int = 3,\n",
        "    device: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a DINOv3 PCA/CLS-style foreground mask for every sample and write to dataset.\n",
        "\n",
        "    - ViT: cosine(sim) to CLS over patch tokens\n",
        "    - ConvNeXt: cosine(sim) to global-avg feature over feature map\n",
        "    - Masks are overlaid natively in the FiftyOne App.\n",
        "    \"\"\"\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    model = AutoModel.from_pretrained(model_id).to(device).eval()\n",
        "\n",
        "    if not dataset.has_field(field):\n",
        "        dataset.add_sample_field(field, fo.EmbeddedDocumentField, embedded_doc_type=fo.Segmentation)\n",
        "\n",
        "    if heatmap_field and not dataset.has_field(heatmap_field):\n",
        "        dataset.add_sample_field(heatmap_field, fo.EmbeddedDocumentField, embedded_doc_type=fo.Heatmap)\n",
        "\n",
        "    mt = dict(dataset.mask_targets or {})\n",
        "    mt[field] = {0: \"background\", 1: \"foreground\"}\n",
        "    dataset.mask_targets = mt\n",
        "    dataset.save()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def _fg_mask(path: str):\n",
        "        img = ImageOps.exif_transpose(Image.open(path).convert(\"RGB\"))\n",
        "        W0, H0 = img.size\n",
        "\n",
        "        bf = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "        last = model(**bf).last_hidden_state\n",
        "\n",
        "        if last.ndim == 3:\n",
        "            hs = last[0].float()\n",
        "            num_reg = getattr(model.config, \"num_register_tokens\", 0)\n",
        "            patch = getattr(model.config, \"patch_size\", 16)\n",
        "            patches = hs[1 + num_reg :, :]\n",
        "            _, _, Hc, Wc = bf[\"pixel_values\"].shape\n",
        "            gh, gw = Hc // patch, Wc // patch\n",
        "\n",
        "            cls = hs[0:1, :]\n",
        "            sims = (F.normalize(patches, dim=1) @ F.normalize(cls, dim=1).T).squeeze(1)\n",
        "            fg = sims.detach().cpu().view(gh, gw)\n",
        "        else:\n",
        "            fm = last[0].float()\n",
        "            C, gh, gw = fm.shape\n",
        "            grid = F.normalize(fm.permute(1, 2, 0).reshape(-1, C), dim=1)\n",
        "            gvec = F.normalize(fm.mean(dim=(1, 2), keepdim=True).squeeze().unsqueeze(0), dim=1)\n",
        "            fg = (grid @ gvec.T).detach().cpu().reshape(gh, gw)\n",
        "\n",
        "        fg01 = (fg - fg.min()) / (fg.max() - fg.min() + 1e-8)\n",
        "\n",
        "        if smooth_k and smooth_k > 1:\n",
        "            fg01 = F.avg_pool2d(fg01.unsqueeze(0).unsqueeze(0), smooth_k, 1, smooth_k // 2).squeeze()\n",
        "\n",
        "        mask_small = (fg01 > thresh).to(torch.uint8).numpy()\n",
        "\n",
        "        mask_full = Image.fromarray(mask_small * 255).resize((W0, H0), Image.NEAREST)\n",
        "        soft_full = Image.fromarray((fg01.numpy() * 255).astype(np.uint8)).resize((W0, H0), Image.BILINEAR)\n",
        "\n",
        "        mask = (np.array(mask_full) > 127).astype(np.uint8)\n",
        "        soft = np.array(soft_full).astype(np.float32) / 255.0\n",
        "        return mask, soft\n",
        "\n",
        "    skipped = 0\n",
        "    for s in dataset.iter_samples(autosave=True, progress=True):\n",
        "        try:\n",
        "            m, soft = _fg_mask(s.filepath)\n",
        "            s[field] = Segmentation(mask=m)\n",
        "            if heatmap_field:\n",
        "                s[heatmap_field] = Heatmap(map=soft)\n",
        "        except Exception:\n",
        "            s[field] = None\n",
        "            if heatmap_field:\n",
        "                s[heatmap_field] = None\n",
        "            skipped += 1\n",
        "\n",
        "    print(f\"\u2713 wrote masks to '{field}'\" + (f\" and heatmaps to '{heatmap_field}'\" if heatmap_field else \"\") + f\". skipped: {skipped}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112c81f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "build_pca_fg_masks(dataset, field=\"pca_fg\", heatmap_field=\"pca_fg_heat\", thresh=0.5, smooth_k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c65dcc5",
      "metadata": {},
      "source": [
        "## Search for a specific label\n",
        "Filter views to samples containing a target label (e.g., `whale`). Use detections or the image-level labels created earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "802f8a79",
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "# Detection-level filter (skip if no detections field)\n",
        "if dataset.has_field(\"detections\"):\n",
        "    whales = dataset.filter_labels(\"detections\", F(\"label\") == \"whale\")\n",
        "    session.view = whales  # optional: show in App\n",
        "    print(\"Detections filtered. Count:\", len(whales))\n",
        "else:\n",
        "    whales = dataset\n",
        "    print(\"Dataset has no detections field; skipping detection-level filter.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6feeb20a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image-level labels (from majority vote step)\n",
        "if dataset.has_field(\"image_label\"):\n",
        "    whales_img = dataset.match(F(\"image_label.label\") == \"whale\")\n",
        "    session.view = whales_img  # optional\n",
        "    print(\"Image-label filter count:\", len(whales_img))\n",
        "else:\n",
        "    whales_img = dataset\n",
        "    print(\"Dataset has no image_label field; skipping image-level filter.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eec10e6",
      "metadata": {},
      "source": [
        "## Foreground-only embeddings (masking background)\n",
        "Use the PCA/CLS mask to zero out background before embedding, then build similarity on foreground-only vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fad6a11",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "fg_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vitl16-pretrain-lvd1689m\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4cd641",
      "metadata": {},
      "outputs": [],
      "source": [
        "def masked_cls_embedding(path, mask, model, processor, device=None):\n",
        "    if mask is None:\n",
        "        return None\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    m = mask\n",
        "    if m.shape != (img.height, img.width):\n",
        "        m = np.array(Image.fromarray(m.astype(np.uint8) * 255).resize((img.width, img.height), Image.NEAREST)) > 127\n",
        "\n",
        "    arr = np.array(img)\n",
        "    arr[~m] = 0\n",
        "    inputs = processor(images=Image.fromarray(arr), return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs).last_hidden_state\n",
        "    cls = out[0, 0].detach().cpu().numpy()\n",
        "    return cls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512677a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "transformers_model = transformers_model.to(device)\n",
        "\n",
        "for sample in dataset.iter_samples(autosave=True, progress=True):\n",
        "    seg = sample[\"pca_fg\"] if sample.has_field(\"pca_fg\") else None\n",
        "    mask = None if seg is None else seg.mask\n",
        "    emb = masked_cls_embedding(sample.filepath, mask, transformers_model, fg_processor, device=device)\n",
        "    sample[\"embeddings_dinov3_fg\"] = emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28256d5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similarity on foreground embeddings\n",
        "fg_idx = fob.compute_similarity(\n",
        "    dataset,\n",
        "    embeddings=\"embeddings_dinov3_fg\",\n",
        "    metric=\"cosine\",\n",
        "    brain_key=\"dino_fg_sim\",\n",
        ")\n",
        "fg_idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb22524",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_id = dataset.first().id\n",
        "fg_view = dataset.sort_by_similarity(query_id, k=30, brain_key=\"dino_fg_sim\")\n",
        "session.view = fg_view\n",
        "fg_view\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9a7c5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: visualize foreground-embedding UMAP\n",
        "fg_viz = fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"embeddings_dinov3_fg\",\n",
        "    brain_key=\"dino_fg_umap\",\n",
        ")\n",
        "fg_viz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster by pose, then by individual (foreground embeddings)\n",
        "Use foreground embeddings (`embeddings_dinov3_fg`) to cluster images by pose first, then sub-cluster each pose to approximate individuals. Adjust `pose_k` and `individual_k` as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "pose_k = 3  # tweak as needed\n",
        "embs = dataset.values(\"embeddings_dinov3_fg\")\n",
        "ids = dataset.values(\"id\")\n",
        "mask = [e is not None for e in embs]\n",
        "X = np.stack([e for e,m in zip(embs,mask) if m], axis=0) if any(mask) else None\n",
        "ids_masked = [i for i,m in zip(ids,mask) if m]\n",
        "\n",
        "if X is None or len(ids_masked) < 2:\n",
        "    print(\"Not enough embeddings for clustering.\")\n",
        "else:\n",
        "    k = min(pose_k, max(2, len(ids_masked)))\n",
        "    kmeans = KMeans(n_clusters=k, random_state=51, n_init=\"auto\").fit(X)\n",
        "    pose_clusters = kmeans.labels_.tolist()\n",
        "    # Assign back to samples in the same order as ids_masked\n",
        "    for sid, lbl in zip(ids_masked, pose_clusters):\n",
        "        sample = dataset[sid]\n",
        "        sample[\"pose_cluster\"] = int(lbl)\n",
        "        sample.save()\n",
        "    print(\"Pose clusters sizes:\", dict(zip(*np.unique(pose_clusters, return_counts=True))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show 5 random examples per pose cluster\n",
        "for pose_id in sorted(set(pose_clusters)):\n",
        "    pose_view = dataset.match(F(\"pose_cluster\") == pose_id)\n",
        "    sample_count = pose_view.count()\n",
        "    few = pose_view.shuffle(seed=pose_id).limit(5)\n",
        "    print(f\"Pose {pose_id}: showing {len(few)} of {sample_count}\")\n",
        "    for s in few:\n",
        "        print(\"    \", s.filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "individual_k = 5  # max clusters per pose; will cap to available samples\n",
        "pose_vals = dataset.values(\"pose_cluster\") if dataset.has_field(\"pose_cluster\") else []\n",
        "embs = dataset.values(\"embeddings_dinov3_fg\")\n",
        "ids = dataset.values(\"id\")\n",
        "\n",
        "if not pose_vals:\n",
        "    print(\"No pose_cluster labels available; run pose clustering first.\")\n",
        "else:\n",
        "    for pose_id in sorted(set(p for p in pose_vals if p is not None)):\n",
        "        pose_embs = [e for e,p in zip(embs, pose_vals) if p == pose_id and e is not None]\n",
        "        pose_ids  = [i for i,p in zip(ids,  pose_vals) if p == pose_id and p is not None]\n",
        "        if len(pose_embs) < 2:\n",
        "            continue\n",
        "        k = min(individual_k, max(2, len(pose_embs)))\n",
        "        kmeans = KMeans(n_clusters=k, random_state=pose_id, n_init=\"auto\").fit(np.stack(pose_embs))\n",
        "        for sid, lbl in zip(pose_ids, kmeans.labels_.tolist()):\n",
        "            sample = dataset[sid]\n",
        "            sample[\"individual_cluster\"] = int(lbl)\n",
        "            sample.save()\n",
        "    print(\"Done assigning individual_cluster labels per pose.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show 5 random examples per inferred individual within each pose\n",
        "for pose_id in sorted(set(pose_clusters)):\n",
        "    pose_view = dataset.match(F(\"pose_cluster\") == pose_id)\n",
        "    if pose_view.count() == 0:\n",
        "        continue\n",
        "    indiv_ids = sorted(set(pose_view.values(\"individual_cluster\")))\n",
        "    print(f\"Pose {pose_id}: {len(indiv_ids)} inferred individuals\")\n",
        "    for cid in indiv_ids:\n",
        "        cluster_view = pose_view.match(F(\"individual_cluster\") == cid)\n",
        "        sample_count = cluster_view.count()\n",
        "        few = cluster_view.shuffle(seed=cid).limit(5)\n",
        "        print(f\"  individual_{cid}: showing {len(few)} of {sample_count}\")\n",
        "        for s in few:\n",
        "            print(\"     \", s.filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "max_poses = 3\n",
        "max_per_pose = 5\n",
        "pose_vals = dataset.values(\"pose_cluster\") if dataset.has_field(\"pose_cluster\") else []\n",
        "if not pose_vals:\n",
        "    print(\"No pose_cluster field; run clustering first.\")\n",
        "else:\n",
        "    pose_ids = sorted(set(p for p in pose_vals if p is not None))[:max_poses]\n",
        "    for pid in pose_ids:\n",
        "        view = dataset.match(F(\"pose_cluster\") == pid)\n",
        "        samples = list(view.shuffle(seed=pid).limit(max_per_pose))\n",
        "        print(f\"Pose {pid}: showing {len(samples)} of {view.count()}\")\n",
        "        if len(samples) == 0:\n",
        "            continue\n",
        "        fig, axes = plt.subplots(1, len(samples), figsize=(4*len(samples), 4))\n",
        "        if len(samples) == 1:\n",
        "            axes = [axes]\n",
        "        for ax, sample in zip(axes, samples):\n",
        "            img = Image.open(sample.filepath).convert(\"RGB\")\n",
        "            ax.imshow(img)\n",
        "            mask = None\n",
        "            if sample.has_field(\"pca_fg\") and sample[\"pca_fg\"] is not None:\n",
        "                mask = sample[\"pca_fg\"].mask\n",
        "            if mask is not None:\n",
        "                ax.imshow(mask, cmap=\"inferno\", alpha=0.35)\n",
        "            ax.set_xticks([]); ax.set_yticks([])\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "max_poses = 3\n",
        "max_per_pose = 5\n",
        "pose_vals = dataset.values(\"pose_cluster\") if dataset.has_field(\"pose_cluster\") else []\n",
        "if not pose_vals:\n",
        "    print(\"No pose_cluster field; run clustering first.\")\n",
        "else:\n",
        "    pose_ids = sorted(set(p for p in pose_vals if p is not None))[:max_poses]\n",
        "    for pid in pose_ids:\n",
        "        view = dataset.match(F(\"pose_cluster\") == pid)\n",
        "        few = view.shuffle(seed=pid).limit(max_per_pose)\n",
        "        print(f\"Pose {pid}: showing {len(few)} of {view.count()}\")\n",
        "        if len(few) == 0:\n",
        "            continue\n",
        "        fig, axes = plt.subplots(1, len(few), figsize=(4*len(few), 4))\n",
        "        if len(few) == 1:\n",
        "            axes = [axes]\n",
        "        for ax, sample in zip(axes, few):\n",
        "            img = Image.open(sample.filepath).convert(\"RGB\")\n",
        "            ax.imshow(img)\n",
        "            mask = None\n",
        "            if sample.has_field(\"pca_fg\") and sample[\"pca_fg\"] is not None:\n",
        "                mask = sample[\"pca_fg\"].mask\n",
        "            if mask is not None:\n",
        "                ax.imshow(mask, cmap=\"inferno\", alpha=0.35)\n",
        "            ax.set_xticks([]); ax.set_yticks([])\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize individual clusters (filepaths)\n",
        "Print up to 5 samples per inferred individual cluster (first 5 clusters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from fiftyone import ViewField as F\n",
        "max_individuals = 5\n",
        "max_per_individual = 5\n",
        "if not dataset.has_field(\"individual_cluster\"):\n",
        "    print(\"No individual_cluster field; run individual clustering first.\")\n",
        "else:\n",
        "    ids = dataset.values(\"individual_cluster\")\n",
        "    clusters = sorted(set(c for c in ids if c is not None))[:max_individuals]\n",
        "    for cid in clusters:\n",
        "        view = dataset.match(F(\"individual_cluster\") == cid)\n",
        "        few = view.shuffle(seed=cid).limit(max_per_individual)\n",
        "        print(f\"individual_{cid}: showing {len(few)} of {view.count()}\")\n",
        "        for s in few:\n",
        "            print(\"   \", s.filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save clustered images to disk\n",
        "Exports copies of clustered images under `runs/<run_name>/pose_clusters` and `runs/<run_name>/individuals` for inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "run_name = \"run1\"  # change per run\n",
        "run_dir = Path(\"runs\") / run_name\n",
        "pose_dir = run_dir / \"pose_clusters\"\n",
        "indiv_dir = run_dir / \"individuals\"\n",
        "pose_dir.mkdir(parents=True, exist_ok=True)\n",
        "indiv_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Export pose clusters\n",
        "if dataset.has_field(\"pose_cluster\"):\n",
        "    pose_vals = dataset.values(\"pose_cluster\")\n",
        "    pose_ids = sorted(set(p for p in pose_vals if p is not None))\n",
        "    for pid in pose_ids:\n",
        "        cluster_dir = pose_dir / f\"pose_{pid}\"\n",
        "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
        "        view = dataset.match(F(\"pose_cluster\") == pid)\n",
        "        for s in view:\n",
        "            dest = cluster_dir / f\"{s.id}_{Path(s.filepath).name}\"\n",
        "            if not dest.exists():\n",
        "                shutil.copy2(s.filepath, dest)\n",
        "    print(f\"Saved pose clusters to {pose_dir}\")\n",
        "else:\n",
        "    print(\"No pose_cluster field; run pose clustering first.\")\n",
        "\n",
        "# Export individual clusters\n",
        "if dataset.has_field(\"individual_cluster\"):\n",
        "    indiv_vals = dataset.values(\"individual_cluster\")\n",
        "    indiv_ids = sorted(set(c for c in indiv_vals if c is not None))\n",
        "    for cid in indiv_ids:\n",
        "        cluster_dir = indiv_dir / f\"individual_{cid}\"\n",
        "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
        "        view = dataset.match(F(\"individual_cluster\") == cid)\n",
        "        for s in view:\n",
        "            dest = cluster_dir / f\"{s.id}_{Path(s.filepath).name}\"\n",
        "            if not dest.exists():\n",
        "                shutil.copy2(s.filepath, dest)\n",
        "    print(f\"Saved individual clusters to {indiv_dir}\")\n",
        "else:\n",
        "    print(\"No individual_cluster field; run individual clustering first.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}