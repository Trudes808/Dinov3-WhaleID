{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DINOv3 Whale Individuals Experiments\n",
        "Minimal notebook to load local images, compute DINOv3 embeddings + foreground masks, cluster by pose/individual, show examples with overlays, and export grouped images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install (if needed)\n",
        "Uncomment to install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install fiftyone jupyter git+https://github.com/huggingface/transformers huggingface_hub torch pillow scikit-learn numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and dataset load from /Data\n",
        "Recursively ingests JPGs under /Data into a FiftyOne dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 images under /Data\n",
            " 100% |█████████████████████| 0/0 [7.7ms elapsed, ? remaining, ? samples/s]  \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Name:        whale-local-dinov3-exp\n",
              "Media type:  None\n",
              "Num samples: 0\n",
              "Persistent:  True\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:               fiftyone.core.fields.ObjectIdField\n",
              "    filepath:         fiftyone.core.fields.StringField\n",
              "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
              "    created_at:       fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at: fiftyone.core.fields.DateTimeField"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import fiftyone as fo\n",
        "from pathlib import Path\n",
        "\n",
        "data_root = Path('/home/trudes/Projects/Dinov3-WhaleID/Data')\n",
        "image_paths = sorted(str(p) for p in data_root.rglob('*.jpg'))\n",
        "print(f'Found {len(image_paths)} images under {data_root}')\n",
        "\n",
        "if fo.dataset_exists('whale-local-dinov3-exp'):\n",
        "    fo.delete_dataset('whale-local-dinov3-exp')\n",
        "dataset = fo.Dataset.from_images(image_paths, name='whale-local-dinov3-exp')\n",
        "dataset.persistent = True\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build DINOv3 model wrapper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5881b9ec54674550834f48b8227070bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/415 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import transformers\n",
        "import fiftyone.utils.transformers as fouhft\n",
        "\n",
        "transformers_model = transformers.AutoModel.from_pretrained('facebook/dinov3-vitl16-pretrain-lvd1689m')\n",
        "model_config = fouhft.FiftyOneTransformerConfig({\n",
        "    'model': transformers_model,\n",
        "    'name_or_path': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "})\n",
        "model = fouhft.FiftyOneTransformer(model_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset.compute_embeddings(model, embeddings_field='embeddings_dinov3')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. PCA/CLS foreground masks + heatmaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from fiftyone import Segmentation, Heatmap\n",
        "\n",
        "def build_pca_fg_masks(\n",
        "    dataset: fo.Dataset,\n",
        "    model_id: str = 'facebook/dinov3-vits16-pretrain-lvd1689m',\n",
        "    field: str = 'pca_fg',\n",
        "    heatmap_field: str | None = 'pca_fg_heat',\n",
        "    thresh: float = 0.5,\n",
        "    smooth_k: int = 3,\n",
        "    device: str | None = None,\n",
        "):\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    m = AutoModel.from_pretrained(model_id).to(device).eval()\n",
        "\n",
        "    if not dataset.has_field(field):\n",
        "        dataset.add_sample_field(field, fo.EmbeddedDocumentField, embedded_doc_type=fo.Segmentation)\n",
        "    if heatmap_field and not dataset.has_field(heatmap_field):\n",
        "        dataset.add_sample_field(heatmap_field, fo.EmbeddedDocumentField, embedded_doc_type=fo.Heatmap)\n",
        "    mt = dict(dataset.mask_targets or {})\n",
        "    mt[field] = {0: 'background', 1: 'foreground'}\n",
        "    dataset.mask_targets = mt\n",
        "    dataset.save()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def _fg_mask(path: str):\n",
        "        img = ImageOps.exif_transpose(Image.open(path).convert('RGB'))\n",
        "        W0, H0 = img.size\n",
        "        bf = processor(images=img, return_tensors='pt').to(device)\n",
        "        last = m(**bf).last_hidden_state\n",
        "        if last.ndim == 3:\n",
        "            hs = last[0].float()\n",
        "            num_reg = getattr(m.config, 'num_register_tokens', 0)\n",
        "            patch = getattr(m.config, 'patch_size', 16)\n",
        "            patches = hs[1 + num_reg :, :]\n",
        "            _, _, Hc, Wc = bf['pixel_values'].shape\n",
        "            gh, gw = Hc // patch, Wc // patch\n",
        "            cls = hs[0:1, :]\n",
        "            sims = (F.normalize(patches, dim=1) @ F.normalize(cls, dim=1).T).squeeze(1)\n",
        "            fg = sims.detach().cpu().view(gh, gw)\n",
        "        else:\n",
        "            fm = last[0].float()\n",
        "            C, gh, gw = fm.shape\n",
        "            grid = F.normalize(fm.permute(1,2,0).reshape(-1, C), dim=1)\n",
        "            gvec = F.normalize(fm.mean(dim=(1,2), keepdim=True).squeeze().unsqueeze(0), dim=1)\n",
        "            fg = (grid @ gvec.T).detach().cpu().reshape(gh, gw)\n",
        "        fg01 = (fg - fg.min()) / (fg.max() - fg.min() + 1e-8)\n",
        "        if smooth_k and smooth_k > 1:\n",
        "            fg01 = F.avg_pool2d(fg01.unsqueeze(0).unsqueeze(0), smooth_k, 1, smooth_k//2).squeeze()\n",
        "        mask_small = (fg01 > thresh).to(torch.uint8).numpy()\n",
        "        mask_full = Image.fromarray(mask_small * 255).resize((W0, H0), Image.NEAREST)\n",
        "        soft_full = Image.fromarray((fg01.numpy() * 255).astype(np.uint8)).resize((W0, H0), Image.BILINEAR)\n",
        "        mask = (np.array(mask_full) > 127).astype(np.uint8)\n",
        "        soft = np.array(soft_full).astype(np.float32) / 255.0\n",
        "        return mask, soft\n",
        "\n",
        "    skipped = 0\n",
        "    for s in dataset.iter_samples(autosave=True, progress=True):\n",
        "        try:\n",
        "            msk, soft = _fg_mask(s.filepath)\n",
        "            s[field] = Segmentation(mask=msk)\n",
        "            if heatmap_field:\n",
        "                s[heatmap_field] = Heatmap(map=soft)\n",
        "        except Exception:\n",
        "            s[field] = None\n",
        "            if heatmap_field:\n",
        "                s[heatmap_field] = None\n",
        "            skipped += 1\n",
        "    msg = \"wrote masks to '{field}'\".format(field=field)\n",
        "    if heatmap_field:\n",
        "        msg += \" and heatmaps to '{hf}'\".format(hf=heatmap_field)\n",
        "    msg += f\". skipped: {skipped}\"\n",
        "    print(msg)\n",
        "\n",
        "build_pca_fg_masks(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Foreground-only CLS embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "fg_processor = AutoImageProcessor.from_pretrained('facebook/dinov3-vitl16-pretrain-lvd1689m')\n",
        "\n",
        "def masked_cls_embedding(path, mask, model, processor, device=None):\n",
        "    if mask is None:\n",
        "        return None\n",
        "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    m = mask\n",
        "    if m.shape != (img.height, img.width):\n",
        "        m = np.array(Image.fromarray(m.astype(np.uint8) * 255).resize((img.width, img.height), Image.NEAREST)) > 127\n",
        "    arr = np.array(img)\n",
        "    arr[~m] = 0\n",
        "    inputs = processor(images=Image.fromarray(arr), return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(**inputs).last_hidden_state\n",
        "    cls = out[0, 0].detach().cpu().numpy()\n",
        "    return cls\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "transformers_model = transformers_model.to(device)\n",
        "\n",
        "for sample in dataset.iter_samples(autosave=True, progress=True):\n",
        "    seg = sample['pca_fg'] if sample.has_field('pca_fg') else None\n",
        "    mask = None if seg is None else seg.mask\n",
        "    emb = masked_cls_embedding(sample.filepath, mask, transformers_model, fg_processor, device=device)\n",
        "    sample['embeddings_dinov3_fg'] = emb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cluster poses and individuals (KMeans on foreground embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "# Pose clustering\n",
        "pose_k = 3\n",
        "embs = dataset.values('embeddings_dinov3_fg')\n",
        "ids = dataset.values('id')\n",
        "mask = [e is not None for e in embs]\n",
        "X = np.stack([e for e,m in zip(embs,mask) if m], axis=0) if any(mask) else None\n",
        "ids_masked = [i for i,m in zip(ids,mask) if m]\n",
        "if X is None or len(ids_masked) < 2:\n",
        "    print('Not enough embeddings for pose clustering.')\n",
        "else:\n",
        "    k = min(pose_k, max(2, len(ids_masked)))\n",
        "    kmeans = KMeans(n_clusters=k, random_state=51, n_init='auto').fit(X)\n",
        "    for sid, lbl in zip(ids_masked, kmeans.labels_.tolist()):\n",
        "        sample = dataset[sid]\n",
        "        sample['pose_cluster'] = int(lbl)\n",
        "        sample.save()\n",
        "    print('Pose clusters sizes:', dict(zip(*np.unique(kmeans.labels_, return_counts=True))))\n",
        "\n",
        "# Individual clustering per pose\n",
        "individual_k = 5\n",
        "pose_vals = dataset.values('pose_cluster') if dataset.has_field('pose_cluster') else []\n",
        "if not pose_vals:\n",
        "    print('No pose_cluster labels available; run pose clustering first.')\n",
        "else:\n",
        "    for pose_id in sorted(set(p for p in pose_vals if p is not None)):\n",
        "        pose_embs = [e for e,p in zip(embs, pose_vals) if p == pose_id and e is not None]\n",
        "        pose_ids  = [i for i,p in zip(ids,  pose_vals) if p == pose_id and p is not None]\n",
        "        if len(pose_embs) < 2:\n",
        "            continue\n",
        "        k = min(individual_k, max(2, len(pose_embs)))\n",
        "        kmeans = KMeans(n_clusters=k, random_state=pose_id, n_init='auto').fit(np.stack(pose_embs))\n",
        "        for sid, lbl in zip(pose_ids, kmeans.labels_.tolist()):\n",
        "            sample = dataset[sid]\n",
        "            sample['individual_cluster'] = int(lbl)\n",
        "            sample.save()\n",
        "    print('Done assigning individual_cluster labels per pose.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize pose examples (mask overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "max_poses = 3\n",
        "max_per_pose = 5\n",
        "pose_vals = dataset.values('pose_cluster') if dataset.has_field('pose_cluster') else []\n",
        "if not pose_vals:\n",
        "    print('No pose_cluster field; run clustering first.')\n",
        "else:\n",
        "    pose_ids = sorted(set(p for p in pose_vals if p is not None))[:max_poses]\n",
        "    for pid in pose_ids:\n",
        "        view = dataset.match(F('pose_cluster') == pid)\n",
        "        samples = list(view.shuffle(seed=pid).limit(max_per_pose))\n",
        "        print(f'Pose {pid}: showing {len(samples)} of {view.count()}')\n",
        "        if len(samples) == 0:\n",
        "            continue\n",
        "        fig, axes = plt.subplots(1, len(samples), figsize=(4*len(samples), 4))\n",
        "        if len(samples) == 1:\n",
        "            axes = [axes]\n",
        "        for ax, sample in zip(axes, samples):\n",
        "            img = Image.open(sample.filepath).convert('RGB')\n",
        "            ax.imshow(img)\n",
        "            mask = None\n",
        "            if sample.has_field('pca_fg') and sample['pca_fg'] is not None:\n",
        "                mask = sample['pca_fg'].mask\n",
        "            if mask is not None:\n",
        "                ax.imshow(mask, cmap='inferno', alpha=0.35)\n",
        "            ax.set_xticks([]); ax.set_yticks([])\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualize individual examples (heatmap overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from fiftyone import ViewField as F\n",
        "import numpy as np\n",
        "\n",
        "max_individuals = 5\n",
        "max_per_individual = 5\n",
        "if not dataset.has_field('individual_cluster'):\n",
        "    print('No individual_cluster field; run individual clustering first.')\n",
        "else:\n",
        "    clusters = sorted(set(c for c in dataset.values('individual_cluster') if c is not None))[:max_individuals]\n",
        "    for cid in clusters:\n",
        "        view = dataset.match(F('individual_cluster') == cid)\n",
        "        samples = list(view.shuffle(seed=cid).limit(max_per_individual))\n",
        "        print(f'individual_{cid}: showing {len(samples)} of {view.count()}')\n",
        "        if len(samples) == 0:\n",
        "            continue\n",
        "        fig, axes = plt.subplots(1, len(samples), figsize=(4*len(samples), 4))\n",
        "        if len(samples) == 1:\n",
        "            axes = [axes]\n",
        "        for ax, sample in zip(axes, samples):\n",
        "            img = Image.open(sample.filepath).convert('RGB')\n",
        "            ax.imshow(img)\n",
        "            heat = None\n",
        "            if sample.has_field('pca_fg_heat') and sample['pca_fg_heat'] is not None:\n",
        "                heat = sample['pca_fg_heat'].map\n",
        "            if heat is not None:\n",
        "                ax.imshow(heat, cmap='inferno', alpha=0.35)\n",
        "            ax.set_xticks([]); ax.set_yticks([])\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export clustered images to runs/run_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "run_name = 'run1'  # change per run\n",
        "run_dir = Path('runs') / run_name\n",
        "pose_dir = run_dir / 'pose_clusters'\n",
        "indiv_dir = run_dir / 'individuals'\n",
        "pose_dir.mkdir(parents=True, exist_ok=True)\n",
        "indiv_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if dataset.has_field('pose_cluster'):\n",
        "    pose_vals = dataset.values('pose_cluster')\n",
        "    pose_ids = sorted(set(p for p in pose_vals if p is not None))\n",
        "    for pid in pose_ids:\n",
        "        cluster_dir = pose_dir / f'pose_{pid}'\n",
        "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
        "        view = dataset.match(F('pose_cluster') == pid)\n",
        "        for s in view:\n",
        "            dest = cluster_dir / f\"{s.id}_{Path(s.filepath).name}\"\n",
        "            if not dest.exists():\n",
        "                shutil.copy2(s.filepath, dest)\n",
        "    print(f'Saved pose clusters to {pose_dir}')\n",
        "else:\n",
        "    print('No pose_cluster field; run pose clustering first.')\n",
        "\n",
        "if dataset.has_field('individual_cluster'):\n",
        "    indiv_vals = dataset.values('individual_cluster')\n",
        "    indiv_ids = sorted(set(c for c in indiv_vals if c is not None))\n",
        "    for cid in indiv_ids:\n",
        "        cluster_dir = indiv_dir / f'individual_{cid}'\n",
        "        cluster_dir.mkdir(parents=True, exist_ok=True)\n",
        "        view = dataset.match(F('individual_cluster') == cid)\n",
        "        for s in view:\n",
        "            dest = cluster_dir / f\"{s.id}_{Path(s.filepath).name}\"\n",
        "            if not dest.exists():\n",
        "                shutil.copy2(s.filepath, dest)\n",
        "    print(f'Saved individual clusters to {indiv_dir}')\n",
        "else:\n",
        "    print('No individual_cluster field; run individual clustering first.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
